# 2.3 Logistic 回归损失函数

> 视频：<https://mooc.study.163.com/learn/deeplearning_ai-2001281002?tid=2001392029#/learn/content?type=detail&id=2001701009&cid=2001694012>

这是我们上一节中的拟合函数`y_hat=sigmoid(w^Tx+b)`。为了找到`w`和`b`，我们需要一个训练集`{x^(i),y^(i)}`。

![](img/2-3-1.jpg)

这里，训练集的预测值写成`y_hat^(i)`，实际值是`y^(i)`，我们自然希望预测值尽可能接近实际值。

你也可以将`z^(i)`定义成这样，`z^(i)`等于`wT*x^(i)+b`，其中上标`(i)`代表第`i`个样本。

![](img/2-3-2.jpg)

接下来，我们需要一个损失函数（或误差函数），它衡量单个样本上预测值和实际值的接近程度。在线性回归中，我们使用`l(y_hat,y)=1/2 (y-y_hat)^2`。但通常在 logistic 回归中不会这样做，因为后面讨论的优化问题可能变成非凸的，也就是存在多个局部最优值，无法使用梯度下降。

![](img/2-3-3.jpg)

在 logistic 回归中，我们会定义，一个不同的损失函数，它的作用和平方误差相似。并且它是凸函数，很容易优化。我们使用`-(y*log(y_hat) + (1-y)log(1-y_hat))`。

![](img/2-3-4.jpg)

对于每个训练样本，损失函数将实际值和预测值相比较，来衡量接近程度。为了更好地理解其作用，我们来看两个例子。

![](img/2-3-5.jpg)

第一个例子中，`y=1`，损失函数正好是第一项，`-log(y_hat)`。如果让损失函数尽量小，`log(y_hat)`就需要足够大，也就是`y_hat`要足够大。由于`y_hat`最大是`1`，所以相当于让`y_hat`接近`1`。

另一个情况就是`y=0`，损失函数正好是第二项，`-log(1-y_hat)`。如果让损失函数尽量小，`log(1-y_hat)`就需要足够大，也就是`y_hat`要足够小。由于`y_hat`最小是`0`，所以相当于让`y_hat`接近`0`。

很多函数都能达到这个效果。后面的选修章节中，会解释为什么选择这个损失函数。

最后，损失函数在单个样本上定义，它衡量了单个样本上的表现。我们需要一个函数，衡量全体样本上的表现。我们让成本函数`J`等于`1/m * ∑l(y_hat^(i),y^(i))`，也就是所有样本的损失函数均值。我们将其展开，是这个样子的：

![](img/2-3-6.jpg)

我们的目的是，找到`w`和`b`，使这个成本函数尽量小。

上面是 logistic 的算法过程，训练样本的损失函数和成本函数。它可看做一个非常小的神经网络，后面会解释如何这样做。
